{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51841126-be07-4a90-ad97-a85c442add07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.2.77\n",
      "Uninstalling llama_cpp_python-0.2.77:\n",
      "  Successfully uninstalled llama_cpp_python-0.2.77\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\ronit khanna\\downloads\\llama_cpp_python-0.2.77-cp311-cp311-win_amd64.whl\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\ronit khanna\\appdata\\roaming\\python\\python311\\site-packages (from llama-cpp-python==0.2.77) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ronit khanna\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python==0.2.77) (2.3.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\ronit khanna\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python==0.2.77) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\ronit khanna\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-cpp-python==0.2.77) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ronit khanna\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.77) (3.0.2)\n",
      "Installing collected packages: llama-cpp-python\n",
      "Successfully installed llama-cpp-python-0.2.77\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (excluding llama-cpp-python for now)\n",
    "%pip install --quiet huggingface_hub\n",
    "%pip install --quiet ipywidgets\n",
    "%pip install --quiet transformers\n",
    "%pip install --quiet joblib\n",
    "%pip install --quiet torch\n",
    "%pip install --quiet sentence-transformers\n",
    "\n",
    "%pip uninstall llama-cpp-python -y\n",
    "%pip install \"C:\\Users\\Ronit Khanna\\Downloads\\llama_cpp_python-0.2.77-cp311-cp311-win_amd64.whl\"\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(new_session=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc1a837-b57c-4154-b0a8-118ac0a36609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN has been set in the environment variables.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the HF_TOKEN environment variable using the secret\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "print(\"HF_TOKEN has been set in the environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c91be-e4f1-4b24-95d7-c65b8eecb63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Mental Health Support Assistant (Enhanced with GoEmotions)\n",
      "============================================================\n",
      "I'm here to listen and support you.\n",
      "I can recognize 27 different emotions to better understand how you're feeling.\n",
      "Type 'quit' or 'exit' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025-08-03 17:05:12,081 - ERROR - Failed to load LLaMA model: Failed to load model from file: ./models/Llama-3.2-3B-Instruct-Q5_K_S.gguf\n",
      "2025-08-03 17:05:12,081 - ERROR - Failed to load LLaMA model: Failed to load model from file: ./models/Llama-3.2-3B-Instruct-Q5_K_S.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully (GoEmotions + Sentiment)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Enhanced Emotion-Sentiment Chatbot with GoEmotions and Guardrails\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================\n",
    "# Enhanced Guardrail Settings with GoEmotions\n",
    "# ====================\n",
    "GOEMOTION_SENTIMENT_SCORE = {\n",
    "    # Very negative emotions\n",
    "    \"sadness\": -4,\n",
    "    \"grief\": -5,\n",
    "    \"disappointment\": -3,\n",
    "    \"anger\": -3,\n",
    "    \"annoyance\": -2,\n",
    "    \"disgust\": -3,\n",
    "    \"fear\": -4,\n",
    "    \"nervousness\": -3,\n",
    "    \"embarrassment\": -2,\n",
    "    \"remorse\": -3,\n",
    "    \n",
    "    # Neutral emotions\n",
    "    \"neutral\": 0,\n",
    "    \"realization\": 0,\n",
    "    \"confusion\": -1,\n",
    "    \"curiosity\": 1,\n",
    "    \"surprise\": 0,\n",
    "    \n",
    "    # Positive emotions\n",
    "    \"joy\": 4,\n",
    "    \"amusement\": 3,\n",
    "    \"approval\": 2,\n",
    "    \"caring\": 3,\n",
    "    \"excitement\": 4,\n",
    "    \"gratitude\": 3,\n",
    "    \"love\": 4,\n",
    "    \"optimism\": 3,\n",
    "    \"pride\": 2,\n",
    "    \"admiration\": 2,\n",
    "    \"desire\": 1,\n",
    "    \"relief\": 2,\n",
    "    \n",
    "    # Contextual emotions (can be positive or negative depending on context)\n",
    "    \"admiration\": 2,\n",
    "    \"desire\": 1,\n",
    "}\n",
    "\n",
    "# Crisis-related emotions that should trigger immediate attention\n",
    "CRISIS_EMOTIONS = [\"grief\", \"sadness\", \"fear\", \"remorse\", \"disappointment\"]\n",
    "\n",
    "CRISIS_KEYWORDS = [\n",
    "    \"suicide\", \"kill myself\", \"self harm\", \"cutting\", \"want to die\", \n",
    "    \"hurt myself\", \"end it all\", \"suicidal\", \"overdose\", \"can't go on\",\n",
    "    \"worthless\", \"hopeless\", \"can't take it anymore\"\n",
    "]\n",
    "\n",
    "THRESHOLD = -25\n",
    "CRISIS_RESOURCES = \"\"\"\n",
    "ðŸ“˜ HELPFUL MENTAL HEALTH RESOURCES (Thapar-Oriented):\n",
    "\n",
    "â€¢ Thapar Institute Counseling Cell Info:\n",
    "  https://www.thapar.edu/index.php?cid=counselling-cell\n",
    "\n",
    "â€¢ Blog: Dealing with Exam Stress (Thapar Students' Perspective):\n",
    "  https://connect.thapar.edu/blog/dealing-with-exam-stress\n",
    "\n",
    "â€¢ Blog: Finding Balance â€“ A Student's Guide to Mental Health:\n",
    "  https://connect.thapar.edu/blog/student-mental-health-guide\n",
    "\n",
    "â€¢ iCall (TISS) Free Counseling via Phone or Email:\n",
    "  https://icallhelpline.org/\n",
    "\n",
    "ðŸ’¡ If you're in immediate distress, please reach out to a trusted friend, mentor, or faculty member.\n",
    "You're not alone, and help is always available.\n",
    "\"\"\"\n",
    "\n",
    "# ====================\n",
    "# Conversation State\n",
    "# ====================\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.score = 0\n",
    "        self.referred = False\n",
    "        self.crisis_count = 0\n",
    "        self.session_start = datetime.now()\n",
    "        self.emotion_pattern = []  # Track emotional patterns\n",
    "\n",
    "    def update(self, user_input, bot_reply, emotions, sentiment):\n",
    "        # Calculate score based on top emotions\n",
    "        score = 0\n",
    "        for emotion_data in emotions:\n",
    "            emotion_label = emotion_data['label']\n",
    "            emotion_score = emotion_data['score']\n",
    "            base_score = GOEMOTION_SENTIMENT_SCORE.get(emotion_label, 0)\n",
    "            score += base_score * emotion_score\n",
    "        \n",
    "        self.score += score\n",
    "        self.emotion_pattern.append([e['label'] for e in emotions[:2]])  # Track top 2 emotions\n",
    "        \n",
    "        self.history.append({\n",
    "            \"user\": user_input,\n",
    "            \"bot\": bot_reply,\n",
    "            \"emotions\": emotions,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"score\": score,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Keep only last 10 exchanges to prevent memory issues\n",
    "        if len(self.history) > 10:\n",
    "            self.history.pop(0)\n",
    "            \n",
    "        # Keep only last 20 emotion patterns\n",
    "        if len(self.emotion_pattern) > 20:\n",
    "            self.emotion_pattern.pop(0)\n",
    "            \n",
    "        return score\n",
    "\n",
    "    def check_crisis(self, text):\n",
    "        text_lower = text.lower()\n",
    "        for keyword in CRISIS_KEYWORDS:\n",
    "            if keyword in text_lower:\n",
    "                self.crisis_count += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def check_crisis_emotions(self, emotions):\n",
    "        \"\"\"Check if current emotions indicate crisis\"\"\"\n",
    "        for emotion_data in emotions:\n",
    "            if emotion_data['label'] in CRISIS_EMOTIONS and emotion_data['score'] > 0.3:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_emotional_trend(self):\n",
    "        \"\"\"Analyze recent emotional trend\"\"\"\n",
    "        if len(self.emotion_pattern) < 3:\n",
    "            return \"insufficient_data\"\n",
    "        \n",
    "        recent_patterns = self.emotion_pattern[-5:]  # Last 5 interactions\n",
    "        negative_count = 0\n",
    "        positive_count = 0\n",
    "        \n",
    "        for pattern in recent_patterns:\n",
    "            for emotion in pattern:\n",
    "                if GOEMOTION_SENTIMENT_SCORE.get(emotion, 0) < -2:\n",
    "                    negative_count += 1\n",
    "                elif GOEMOTION_SENTIMENT_SCORE.get(emotion, 0) > 2:\n",
    "                    positive_count += 1\n",
    "        \n",
    "        if negative_count > positive_count * 2:\n",
    "            return \"declining\"\n",
    "        elif positive_count > negative_count * 2:\n",
    "            return \"improving\"\n",
    "        else:\n",
    "            return \"stable\"\n",
    "\n",
    "    def log_referral(self):\n",
    "        try:\n",
    "            os.makedirs(\"logs\", exist_ok=True)\n",
    "            with open(\"logs/support_referrals.log\", \"a\", encoding='utf-8') as logf:\n",
    "                logf.write(f\"[{datetime.now().isoformat()}] URGENT SUPPORT REFERRAL\\n\")\n",
    "                logf.write(f\"Session Duration: {datetime.now() - self.session_start}\\n\")\n",
    "                logf.write(f\"Cumulative Score: {self.score}\\n\")\n",
    "                logf.write(f\"Crisis Keywords Detected: {self.crisis_count}\\n\")\n",
    "                logf.write(f\"Emotional Trend: {self.get_emotional_trend()}\\n\")\n",
    "                logf.write(f\"Recent Emotion Pattern: {self.emotion_pattern[-5:]}\\n\")\n",
    "                logf.write(f\"Recent History: {json.dumps(self.history[-3:], indent=2)}\\n\")\n",
    "                logf.write(\"-\" * 50 + \"\\n\\n\")\n",
    "            self.referred = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log referral: {e}\")\n",
    "\n",
    "# ====================\n",
    "# LLaMA Responder\n",
    "# ====================\n",
    "class LlamaResponder:\n",
    "    def __init__(self, model_path=\"./models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf\"):\n",
    "        try:\n",
    "            self.model = Llama(\n",
    "                model_path=model_path,\n",
    "                n_gpu_layers=32,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True,\n",
    "                n_threads=8,\n",
    "                f16_kv=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            self.model_loaded = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load LLaMA model: {e}\")\n",
    "            self.model_loaded = False\n",
    "\n",
    "    def generate_response(self, user_input, emotions, sentiment, history, emotional_trend):\n",
    "        if not self.model_loaded:\n",
    "            return \"I'm here to listen and support you. How are you feeling right now?\"\n",
    "        \n",
    "        try:\n",
    "            # Build context from recent history\n",
    "            context = \"\"\n",
    "            if history:\n",
    "                recent_history = history[-3:]  # Last 3 exchanges\n",
    "                for exchange in recent_history:\n",
    "                    context += f\"User: {exchange['user']}\\nAI: {exchange['bot']}\\n\"\n",
    "            \n",
    "            # Format emotions for context\n",
    "            emotion_context = \", \".join([f\"{e['label']} ({e['score']:.2f})\" for e in emotions[:3]])\n",
    "            \n",
    "            # Enhanced prompt with emotion awareness\n",
    "            prompt = f\"\"\"You are a compassionate, trained mental health supporter. Be warm, empathetic, and supportive.\n",
    "\n",
    "Current user emotions: {emotion_context}\n",
    "Current sentiment: {sentiment}\n",
    "Emotional trend: {emotional_trend}\n",
    "\n",
    "Previous conversation:\n",
    "{context}\n",
    "\n",
    "User: {user_input}\n",
    "\n",
    "Respond with empathy, validation, and gentle support. Keep your response under 100 words.\n",
    "AI:\"\"\"\n",
    "\n",
    "            output = self.model(\n",
    "                prompt.strip(), \n",
    "                max_tokens=120, \n",
    "                stop=[\"User:\", \"AI:\", \"\\n\\n\"], \n",
    "                echo=False,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "            response = output[\"choices\"][0][\"text\"].strip()\n",
    "            \n",
    "            # Ensure response isn't empty\n",
    "            if not response:\n",
    "                return \"I hear you, and I want you to know that your feelings are valid. Can you tell me more about what's on your mind?\"\n",
    "                \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {e}\")\n",
    "            return \"I'm here for you. Sometimes it helps to talk about what's bothering you. How can I support you today?\"\n",
    "\n",
    "# ====================\n",
    "# Enhanced Emotion Pipeline with GoEmotions\n",
    "# ====================\n",
    "class EmotionSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            \"bert_sentiment_model_path\": \"./best_sentiment_model\",\n",
    "            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        }\n",
    "        self.sentiment_id_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        self.models_loaded = False\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        try:\n",
    "            # Load GoEmotions pipeline\n",
    "            self.goemotion_pipe = pipeline(\n",
    "                \"text-classification\", \n",
    "                model=\"bhadresh-savani/bert-base-go-emotion\", \n",
    "                top_k=None,  # Return all labels with scores\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Load sentiment model (keeping your existing one) - Fixed with local_files_only=True\n",
    "            self.sentiment_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config[\"bert_sentiment_model_path\"], \n",
    "                local_files_only=True\n",
    "            )\n",
    "            self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.config[\"bert_sentiment_model_path\"], \n",
    "                local_files_only=True\n",
    "            ).to(self.config[\"device\"]).eval()\n",
    "\n",
    "            # Initialize responder\n",
    "            self.llama_responder = LlamaResponder()\n",
    "            \n",
    "            self.models_loaded = True\n",
    "            print(\"All models loaded successfully (GoEmotions + Sentiment)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {e}\")\n",
    "            print(f\"Model loading failed: {e}\")\n",
    "            self.models_loaded = False\n",
    "\n",
    "    def get_top_emotions(self, text, top_k=3, threshold=0.05):\n",
    "        \"\"\"Get top emotions using GoEmotions model\"\"\"\n",
    "        if not self.models_loaded:\n",
    "            return [{'label': 'neutral', 'score': 1.0}]\n",
    "            \n",
    "        try:\n",
    "            results = self.goemotion_pipe(text)[0]  # Unwrap the outer list\n",
    "            # Sort by score and filter by threshold\n",
    "            top_results = sorted(results, key=lambda x: x['score'], reverse=True)\n",
    "            filtered = [r for r in top_results if r['score'] >= threshold]\n",
    "            # Return top-k (filtered) results\n",
    "            return filtered[:top_k] if filtered else [{'label': 'neutral', 'score': 1.0}]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting emotions: {e}\")\n",
    "            return [{'label': 'neutral', 'score': 1.0}]\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        if not self.models_loaded:\n",
    "            return \"Neutral\"\n",
    "            \n",
    "        try:\n",
    "            inputs = self.sentiment_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.config[\"device\"]) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.sentiment_model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                pred_id = torch.argmax(probs, dim=-1).item()\n",
    "            return self.sentiment_id_to_label.get(pred_id, \"Neutral\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting sentiment: {e}\")\n",
    "            return \"Neutral\"\n",
    "\n",
    "    def analyze_text(self, text, state):\n",
    "        # Handle empty input\n",
    "        if not text or not text.strip():\n",
    "            return \"I'm here when you're ready to talk. Take your time.\"\n",
    "        \n",
    "        # Crisis detection with immediate response\n",
    "        if state.check_crisis(text):\n",
    "            state.log_referral()\n",
    "            crisis_response = (\n",
    "                \"I'm really concerned about you right now. Your life has value, and there are people who want to help. \"\n",
    "                \"Please reach out to a crisis helpline or emergency services immediately.\\n\\n\" + CRISIS_RESOURCES\n",
    "            )\n",
    "            return crisis_response\n",
    "\n",
    "        # Emotion and sentiment analysis\n",
    "        emotions = self.get_top_emotions(text, top_k=3, threshold=0.05)\n",
    "        sentiment = self.predict_sentiment(text)\n",
    "        \n",
    "        # Check for crisis-related emotions\n",
    "        if state.check_crisis_emotions(emotions):\n",
    "            state.log_referral()\n",
    "            crisis_response = (\n",
    "                \"I can sense you're going through something really difficult right now. \"\n",
    "                \"Your feelings are valid, and you don't have to face this alone. \"\n",
    "                \"Please consider reaching out for professional support:\\n\\n\" + CRISIS_RESOURCES\n",
    "            )\n",
    "            return crisis_response\n",
    "\n",
    "        # Get emotional trend\n",
    "        emotional_trend = state.get_emotional_trend()\n",
    "\n",
    "        # Generate response\n",
    "        bot_reply = self.llama_responder.generate_response(\n",
    "            text, emotions, sentiment, state.history, emotional_trend\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        score_change = state.update(text, bot_reply, emotions, sentiment)\n",
    "\n",
    "        # Check for referral threshold\n",
    "        if state.score <= THRESHOLD and not state.referred:\n",
    "            state.log_referral()\n",
    "            referral_note = \"\\n\\nI notice you've been struggling consistently. A mental health professional can provide specialized support that might be really helpful right now. You deserve care and support.\"\n",
    "            return bot_reply + referral_note\n",
    "\n",
    "        # Add gentle encouragement for improving trends\n",
    "        if emotional_trend == \"improving\" and state.score > -10:\n",
    "            encouragement = \"\\n\\nI'm noticing some positive changes in how you're feeling. That's really encouraging.\"\n",
    "            return bot_reply + encouragement\n",
    "\n",
    "        return bot_reply\n",
    "\n",
    "# ====================\n",
    "# Run Interactive Chat\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup logging\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('logs/chatbot_errors.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ§  Mental Health Support Assistant (Enhanced with GoEmotions)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"I'm here to listen and support you.\")\n",
    "    print(\"I can recognize 27 different emotions to better understand how you're feeling.\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\\n\")\n",
    "    \n",
    "    try:\n",
    "        pipeline = EmotionSentimentPipeline()\n",
    "        state = ConversationState()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"AI: Take care of yourself. Remember, you're not alone, and your feelings matter. ðŸ’™\")\n",
    "                    break\n",
    "                    \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                # Show detected emotions for transparency (optional - remove in production)\n",
    "                emotions = pipeline.get_top_emotions(user_input)\n",
    "                \n",
    "                    \n",
    "                response = pipeline.analyze_text(user_input, state)\n",
    "                print(f\"AI: {response}\\n\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nAI: Take care of yourself. Remember, you're not alone. ðŸ’™\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in main loop: {e}\")\n",
    "                print(\"AI: I'm having some technical difficulties, but I'm still here for you. How are you feeling?\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Critical error initializing chatbot: {e}\")\n",
    "        print(f\"Sorry, I'm having trouble starting up. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8525d-f8d7-4c0c-ad54-291a4e7c6762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
