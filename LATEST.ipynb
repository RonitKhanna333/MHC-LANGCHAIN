{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73200e5-7714-46b9-a5f1-31a63fcc0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0756bd86-dbe8-4139-8a53-c7766393c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN has been set in the environment variables.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the HF_TOKEN environment variable using the secret\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "print(\"HF_TOKEN has been set in the environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1575a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Mental Health Support Assistant (Memory Optimized)\n",
      "============================================================\n",
      "I'm here to listen and support you.\n",
      "Type 'quit' or 'exit' to end the conversation.\n",
      "\n",
      "üîÑ Loading models (memory optimized)...\n",
      "‚úÖ BERT models loaded successfully\n",
      "‚úÖ TinyLlama model loaded successfully (memory optimized)\n",
      "‚úÖ All models initialized successfully\n",
      "‚úÖ BERT models loaded successfully\n",
      "‚úÖ TinyLlama model loaded successfully (memory optimized)\n",
      "‚úÖ All models initialized successfully\n",
      "AI: Hello! I'm here to listen and support you. How are you feeling today?\n",
      "\n",
      "AI: Hello! I'm here to listen and support you. How are you feeling today?\n",
      "\n",
      "AI: Hi there! I'm glad you reached out. What's on your mind?\n",
      "\n",
      "AI: Hi there! I'm glad you reached out. What's on your mind?\n",
      "\n",
      "AI: I understand, do you want to talk about it?\n",
      "Previous:\n",
      "\n",
      "AI: I understand, do you want to talk about it?\n",
      "Previous:\n",
      "\n",
      "\n",
      "\n",
      "AI: Take care of yourself. Remember, you're not alone. üíô\n",
      "\n",
      "\n",
      "AI: Take care of yourself. Remember, you're not alone. üíô\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Enhanced Emotion-Sentiment Chatbot with TinyLlama Model\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from llama_cpp import Llama\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================\n",
    "# Guardrail Settings\n",
    "# ====================\n",
    "\n",
    "EMOTION_SENTIMENT_SCORE = {\n",
    "    # üå©Ô∏è Negative Emotions\n",
    "    (\"Angry\", \"Negative\"): -3,\n",
    "    (\"Sad\", \"Negative\"): -4,\n",
    "    (\"Annoyed\", \"Negative\"): -3,\n",
    "    (\"Lonely\", \"Negative\"): -4,\n",
    "    (\"Afraid\", \"Negative\"): -4,\n",
    "    (\"Terrified\", \"Negative\"): -5,\n",
    "    (\"Guilty\", \"Negative\"): -4,\n",
    "    (\"Disgusted\", \"Negative\"): -3,\n",
    "    (\"Furious\", \"Negative\"): -4,\n",
    "    (\"Anxious\", \"Negative\"): -4,\n",
    "    (\"Disappointed\", \"Negative\"): -3,\n",
    "    (\"Jealous\", \"Negative\"): -2,\n",
    "    (\"Ashamed\", \"Negative\"): -4,\n",
    "    (\"Embarrassed\", \"Negative\"): -2,\n",
    "    (\"Devastated\", \"Negative\"): -5,\n",
    "    (\"Apprehensive\", \"Negative\"): -3,\n",
    "\n",
    "    # ‚öñÔ∏è Neutral or Mixed Emotions\n",
    "    (\"Prepared\", \"Neutral\"): 1,\n",
    "    (\"Impressed\", \"Neutral\"): 2,\n",
    "    (\"Surprised\", \"Neutral\"): 1,\n",
    "    (\"Nostalgic\", \"Neutral\"): 1,\n",
    "    (\"Trusting\", \"Neutral\"): 2,\n",
    "    (\"Content\", \"Neutral\"): 2,\n",
    "    (\"Proud\", \"Neutral\"): 3,\n",
    "\n",
    "    # üå§Ô∏è Positive Emotions\n",
    "    (\"Grateful\", \"Positive\"): 4,\n",
    "    (\"Hopeful\", \"Positive\"): 3,\n",
    "    (\"Confident\", \"Positive\"): 3,\n",
    "    (\"Excited\", \"Positive\"): 4,\n",
    "    (\"Joyful\", \"Positive\"): 5,\n",
    "    (\"Caring\", \"Positive\"): 3,\n",
    "    (\"Faithful\", \"Positive\"): 3,\n",
    "}\n",
    "\n",
    "CRISIS_KEYWORDS = [\n",
    "    \"suicide\", \"kill myself\", \"self harm\", \"cutting\", \"want to die\",\"kill\" \n",
    "    \"hurt myself\", \"end it all\", \"suicidal\", \"overdose\", \"can't go on\"\n",
    "]\n",
    "\n",
    "# Special neutral greetings and basic interactions\n",
    "NEUTRAL_GREETINGS = [\n",
    "    \"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\", \"good evening\",\n",
    "    \"how are you\", \"what's up\", \"sup\", \"greetings\", \"hiya\", \"howdy\",\n",
    "    \"good day\", \"thanks\", \"thank you\", \"bye\", \"goodbye\", \"see you\",\n",
    "    \"take care\", \"have a good day\", \"nice to meet you\", \"pleasure to meet you\",\n",
    "    \"how do you do\", \"what's going on\", \"how's it going\", \"how have you been\",\n",
    "    \"long time no see\", \"good to see you\", \"nice seeing you\", \"hi there\",\n",
    "    \"hello there\", \"hey there\", \"good to hear from you\", \"nice to talk to you\"\n",
    "]\n",
    "\n",
    "THRESHOLD = -30\n",
    "CRISIS_RESOURCES = \"\"\"\n",
    "üìò HELPFUL MENTAL HEALTH RESOURCES (Thapar-Oriented):\n",
    "\n",
    "‚Ä¢ Thapar Institute Counseling Cell Info:\n",
    "  https://www.thapar.edu/index.php?cid=counselling-cell\n",
    "\n",
    "‚Ä¢ Blog: Dealing with Exam Stress (Thapar Students' Perspective):\n",
    "  https://connect.thapar.edu/blog/dealing-with-exam-stress\n",
    "\n",
    "‚Ä¢ Blog: Finding Balance ‚Äì A Student's Guide to Mental Health:\n",
    "  https://connect.thapar.edu/blog/student-mental-health-guide\n",
    "\n",
    "‚Ä¢ iCall (TISS) Free Counseling via Phone or Email:\n",
    "  https://icallhelpline.org/\n",
    "\n",
    "üí° If you're in immediate distress, please reach out to a trusted friend, mentor, or faculty member.\n",
    "You're not alone, and help is always available.\n",
    "\"\"\"\n",
    "\n",
    "# ====================\n",
    "# Conversation State\n",
    "# ====================\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.score = 0\n",
    "        self.referred = False\n",
    "        self.crisis_count = 0\n",
    "        self.session_start = datetime.now()\n",
    "\n",
    "    def update(self, user_input, bot_reply, emotion, sentiment):\n",
    "        score = EMOTION_SENTIMENT_SCORE.get((emotion, sentiment), 0)\n",
    "        self.score += score\n",
    "        self.history.append({\n",
    "            \"user\": user_input,\n",
    "            \"bot\": bot_reply,\n",
    "            \"emotion\": emotion,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"score\": score,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Keep only last 10 exchanges to prevent memory issues\n",
    "        if len(self.history) > 10:\n",
    "            self.history.pop(0)\n",
    "            \n",
    "        return score\n",
    "\n",
    "    def check_crisis(self, text):\n",
    "        text_lower = text.lower()\n",
    "        for keyword in CRISIS_KEYWORDS:\n",
    "            if keyword in text_lower:\n",
    "                self.crisis_count += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def log_referral(self):\n",
    "        try:\n",
    "            os.makedirs(\"logs\", exist_ok=True)\n",
    "            with open(\"logs/support_referrals.log\", \"a\", encoding='utf-8') as logf:\n",
    "                logf.write(f\"[{datetime.now().isoformat()}] URGENT SUPPORT REFERRAL\\n\")\n",
    "                logf.write(f\"Session Duration: {datetime.now() - self.session_start}\\n\")\n",
    "                logf.write(f\"Cumulative Score: {self.score}\\n\")\n",
    "                logf.write(f\"Crisis Keywords Detected: {self.crisis_count}\\n\")\n",
    "                logf.write(f\"Recent History: {json.dumps(self.history[-3:], indent=2)}\\n\")\n",
    "                logf.write(\"-\" * 50 + \"\\n\\n\")\n",
    "            self.referred = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log referral: {e}\")\n",
    "\n",
    "# ====================\n",
    "# TinyLlama Responder - MEMORY OPTIMIZED\n",
    "# ====================\n",
    "class LlamaResponder:\n",
    "    def __init__(self, model_path=\"models/llama-7b.Q2_K.gguf\"):\n",
    "        self.model = None\n",
    "        self.model_loaded = False\n",
    "        self.model_path = model_path\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            # Check if model file exists\n",
    "            if not os.path.exists(self.model_path):\n",
    "                print(f\"‚ö†Ô∏è Model file not found: {self.model_path}\")\n",
    "                print(\"Falling back to pre-defined responses...\")\n",
    "                return\n",
    "            \n",
    "            # Memory-optimized settings for TinyLlama\n",
    "            self.model = Llama(\n",
    "                model_path=self.model_path,\n",
    "                n_gpu_layers=0,  # Use CPU only to reduce memory usage\n",
    "                n_ctx=1024,      # Reduced context window\n",
    "                use_mlock=False, # Disable mlock to reduce memory pressure\n",
    "                n_threads=4,     # Reduced thread count\n",
    "                f16_kv=False,    # Use int8 instead of fp16 for key-value cache\n",
    "                verbose=False\n",
    "            )\n",
    "            self.model_loaded = True\n",
    "            print(f\"‚úÖ TinyLlama model loaded successfully (memory optimized)\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load TinyLlama model: {e}\")\n",
    "            self.model_loaded = False\n",
    "            print(f\"‚ùå Failed to load TinyLlama model: {e}\")\n",
    "            print(\"Using fallback response system...\")\n",
    "\n",
    "    def generate_response(self, user_input, emotion, sentiment, history, top_emotions=None):\n",
    "        # Fallback responses if model not loaded\n",
    "        if not self.model_loaded:\n",
    "            fallback_responses = {\n",
    "                \"Sad\": \"I can hear the sadness in your words. It's okay to feel this way. Would you like to talk about what's making you feel sad?\",\n",
    "                \"Angry\": \"It sounds like you're feeling really frustrated right now. Those feelings are valid. What's been bothering you?\",\n",
    "                \"Anxious\": \"I can sense you're feeling worried or anxious. That must be really uncomfortable. Can you tell me what's on your mind?\",\n",
    "                \"Lonely\": \"Feeling alone can be really painful. I want you to know that I'm here with you right now. You're not as alone as you might feel.\",\n",
    "                \"Afraid\": \"It takes courage to share when you're feeling scared. I'm here to listen. What's making you feel afraid?\",\n",
    "                \"Guilty\": \"Guilt can be such a heavy feeling. Remember that everyone makes mistakes, and you deserve compassion - including from yourself.\",\n",
    "                \"Disappointed\": \"Disappointment can be really hard to handle. It's okay to feel let down. Would you like to talk about what happened?\",\n",
    "                \"default\": \"I hear you, and I want you to know that your feelings are valid. Can you tell me more about what's on your mind?\"\n",
    "            }\n",
    "            return fallback_responses.get(emotion, fallback_responses[\"default\"])\n",
    "        \n",
    "        try:\n",
    "            # Build minimal context to save memory\n",
    "            context = \"\"\n",
    "            if history and len(history) > 0:\n",
    "                # Only use the last exchange for context\n",
    "                last_exchange = history[-1]\n",
    "                context = f\"Previous: User said '{last_exchange['user'][:50]}...', AI responded supportively.\\n\"\n",
    "            \n",
    "            # Simplified prompt for memory efficiency\n",
    "            prompt = f\"\"\"You are a supportive mental health assistant. User feels {emotion} ({sentiment}). Be warm and empathetic.\n",
    "\n",
    "{context}User: {user_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "            output = self.model(\n",
    "                prompt.strip(), \n",
    "                max_tokens=80,  # Reduced for memory efficiency\n",
    "                stop=[\"User:\", \"\\n\\n\"], \n",
    "                echo=False,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "            response = output[\"choices\"][0][\"text\"].strip()\n",
    "            \n",
    "            # Ensure response isn't empty\n",
    "            if not response:\n",
    "                return \"I hear you, and I want you to know that your feelings are valid. Can you tell me more about what's on your mind?\"\n",
    "                \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {e}\")\n",
    "            return \"I'm here for you. Sometimes it helps to talk about what's bothering you. How can I support you today?\"\n",
    "\n",
    "# ====================\n",
    "# Emotion Sentiment Pipeline - MEMORY OPTIMIZED\n",
    "# ====================\n",
    "class EmotionSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            \"bert_emotion_model_path\": \"./best_model\",\n",
    "            \"bert_sentiment_model_path\": \"./best_sentiment_model\",\n",
    "            \"emotion_label_encoder_path\": \"./label_encoder.pkl\",\n",
    "            \"device\": \"cpu\"  # Force CPU to reduce memory usage\n",
    "        }\n",
    "        self.sentiment_id_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        self.models_loaded = False\n",
    "        self.llama_responder = None\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        try:\n",
    "            print(\"üîÑ Loading models (memory optimized)...\")\n",
    "            \n",
    "            # Clear any existing GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Load emotion model with local_files_only\n",
    "            self.emotion_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config[\"bert_emotion_model_path\"], \n",
    "                local_files_only=True\n",
    "            )\n",
    "            self.emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.config[\"bert_emotion_model_path\"], \n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.float32  # Use float32 for better CPU compatibility\n",
    "            ).to(self.config[\"device\"]).eval()\n",
    "\n",
    "            # Load emotion label encoder\n",
    "            self.emotion_label_encoder = joblib.load(self.config[\"emotion_label_encoder_path\"])\n",
    "\n",
    "            # Load sentiment model with local_files_only\n",
    "            self.sentiment_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config[\"bert_sentiment_model_path\"], \n",
    "                local_files_only=True\n",
    "            )\n",
    "            self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.config[\"bert_sentiment_model_path\"], \n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.float32  # Use float32 for better CPU compatibility\n",
    "            ).to(self.config[\"device\"]).eval()\n",
    "\n",
    "            print(\"‚úÖ BERT models loaded successfully\")\n",
    "            \n",
    "            # Initialize responder after BERT models are loaded\n",
    "            self.llama_responder = LlamaResponder()\n",
    "            \n",
    "            self.models_loaded = True\n",
    "            print(\"‚úÖ All models initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {e}\")\n",
    "            print(f\"‚ùå Model loading failed: {e}\")\n",
    "            print(\"Will use fallback response system...\")\n",
    "            self.models_loaded = False\n",
    "            \n",
    "            # Still initialize responder for fallback responses\n",
    "            self.llama_responder = LlamaResponder()\n",
    "\n",
    "    def predict_emotion_top3(self, text):\n",
    "        \"\"\"Predict top 3 emotions with their probabilities\"\"\"\n",
    "        if not self.models_loaded:\n",
    "            return [\"Content\"], [1.0]\n",
    "            \n",
    "        try:\n",
    "            inputs = self.emotion_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.config[\"device\"]) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.emotion_model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                # Get top 3 predictions\n",
    "                top3_probs, top3_indices = torch.topk(probs, 3, dim=-1)\n",
    "                top3_probs = top3_probs.squeeze().cpu().numpy()\n",
    "                top3_indices = top3_indices.squeeze().cpu().numpy()\n",
    "                \n",
    "                # Convert indices to emotion labels\n",
    "                top3_emotions = []\n",
    "                for idx in top3_indices:\n",
    "                    emotion_label = self.emotion_label_encoder.inverse_transform([idx])[0]\n",
    "                    top3_emotions.append(emotion_label)\n",
    "                \n",
    "                return top3_emotions, top3_probs\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting emotion: {e}\")\n",
    "            return [\"Content\"], [1.0]\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        if not self.models_loaded:\n",
    "            return \"Neutral\"\n",
    "            \n",
    "        try:\n",
    "            inputs = self.sentiment_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.config[\"device\"]) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.sentiment_model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                pred_id = torch.argmax(probs, dim=-1).item()\n",
    "            return self.sentiment_id_to_label.get(pred_id, \"Neutral\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting sentiment: {e}\")\n",
    "            return \"Neutral\"\n",
    "\n",
    "    def analyze_text(self, text, state):\n",
    "        # Handle empty input\n",
    "        if not text or not text.strip():\n",
    "            return \"I'm here when you're ready to talk. Take your time.\"\n",
    "        \n",
    "        # Crisis detection with immediate response\n",
    "        if state.check_crisis(text):\n",
    "            state.log_referral()\n",
    "            crisis_response = (\n",
    "                \"I'm really concerned about you right now. Your life has value, and there are people who want to help. \"\n",
    "                \"Please reach out to a crisis helpline or emergency services immediately.\\n\\n\" + CRISIS_RESOURCES\n",
    "            )\n",
    "            return crisis_response\n",
    "\n",
    "        # Check for basic greetings and neutral interactions\n",
    "        text_lower = text.lower().strip()\n",
    "        if any(greeting in text_lower for greeting in NEUTRAL_GREETINGS):\n",
    "            # Handle greetings with neutral responses\n",
    "            greeting_responses = [\n",
    "                \"Hello! I'm here to listen and support you. How are you feeling today?\",\n",
    "                \"Hi there! I'm glad you reached out. What's on your mind?\",\n",
    "                \"Hello! I'm here for you. How can I support you today?\",\n",
    "                \"Hi! It's good to connect with you. How are you doing?\",\n",
    "                \"Hello! I'm here to listen. What would you like to talk about?\"\n",
    "            ]\n",
    "            \n",
    "            # Simple rotation based on conversation length\n",
    "            response_index = len(state.history) % len(greeting_responses)\n",
    "            bot_reply = greeting_responses[response_index]\n",
    "            \n",
    "            # Update state with neutral emotion for greetings\n",
    "            state.update(text, bot_reply, \"Content\", \"Neutral\")\n",
    "            return bot_reply\n",
    "\n",
    "        # Emotion and sentiment analysis for non-greeting messages\n",
    "        top_emotions, emotion_probs = self.predict_emotion_top3(text)\n",
    "        primary_emotion = top_emotions[0]  # Use top emotion as primary\n",
    "        sentiment = self.predict_sentiment(text)\n",
    "\n",
    "        # Generate response with top emotions context\n",
    "        if self.llama_responder:\n",
    "            bot_reply = self.llama_responder.generate_response(\n",
    "                text, primary_emotion, sentiment, state.history, top_emotions\n",
    "            )\n",
    "        else:\n",
    "            bot_reply = \"I'm here to listen and support you. How are you feeling right now?\"\n",
    "        \n",
    "        # Update state with primary emotion\n",
    "        score_change = state.update(text, bot_reply, primary_emotion, sentiment)\n",
    "\n",
    "        # Check for referral threshold\n",
    "        if state.score <= THRESHOLD and not state.referred:\n",
    "            state.log_referral()\n",
    "            referral_note = \"\\n\\nI notice you've been struggling. Consider reaching out to a mental health professional who can provide specialized support. You deserve care and support.\"\n",
    "            return bot_reply + referral_note\n",
    "\n",
    "        return bot_reply\n",
    "\n",
    "# ====================\n",
    "# Run Interactive Chat - MEMORY OPTIMIZED\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup logging\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('logs/chatbot_errors.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"üß† Mental Health Support Assistant (Memory Optimized)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"I'm here to listen and support you.\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\\n\")\n",
    "    \n",
    "    try:\n",
    "        pipeline = EmotionSentimentPipeline()\n",
    "        state = ConversationState()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"AI: Take care of yourself. Remember, you're not alone. üíô\")\n",
    "                    break\n",
    "                    \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                    \n",
    "                response = pipeline.analyze_text(user_input, state)\n",
    "                print(f\"AI: {response}\\n\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nAI: Take care of yourself. Remember, you're not alone. üíô\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in main loop: {e}\")\n",
    "                print(\"AI: I'm having some technical difficulties, but I'm still here for you. How are you feeling?\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Critical error initializing chatbot: {e}\")\n",
    "        print(f\"Sorry, I'm having trouble starting up. Error: {e}\")\n",
    "        print(\"The system will still try to help using fallback responses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
