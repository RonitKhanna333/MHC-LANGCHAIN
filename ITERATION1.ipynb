{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bf5c3d-af59-4c22-9066-5982ae16d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7251a818-aa64-4f9a-bd35-b66a9823cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN has been set in the environment variables.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the HF_TOKEN environment variable using the secret\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "print(\"HF_TOKEN has been set in the environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167a6f5e-7e87-42d8-b8be-d348659f1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Mental Health Support Assistant\n",
      "========================================\n",
      "I'm here to listen and support you.\n",
      "Type 'quit' or 'exit' to end the conversation.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Welcome! I'm here to listen and support you. How are you feeling today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Not very good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm so sorry to hear that. It sounds like you're struggling with something that's making you feel really scared and uncertain. Would you like to talk about what's going on and how you're feeling?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Take care of yourself. Remember, you're not alone. ðŸ’™\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Enhanced Emotion-Sentiment Chatbot with Guardrails\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ====================\n",
    "# Guardrail Settings\n",
    "# ====================\n",
    "EMOTION_SENTIMENT_SCORE = {\n",
    "    (\"devastated\", \"Negative\"): -5,\n",
    "    (\"anxious\", \"Negative\"): -4,\n",
    "    (\"lonely\", \"Negative\"): -4,\n",
    "    (\"disappointed\", \"Negative\"): -3,\n",
    "    (\"angry\", \"Negative\"): -3,\n",
    "    (\"sad\", \"Neutral\"): -2,\n",
    "    (\"hopeful\", \"Neutral\"): 1,\n",
    "    (\"calm\", \"Positive\"): 3,\n",
    "    (\"caring\", \"Positive\"): 2,\n",
    "    (\"joyful\", \"Positive\"): 5,\n",
    "}\n",
    "\n",
    "CRISIS_KEYWORDS = [\n",
    "    \"suicide\", \"kill myself\", \"self harm\", \"cutting\", \"want to die\", \n",
    "    \"hurt myself\", \"end it all\", \"suicidal\", \"overdose\", \"can't go on\"\n",
    "]\n",
    "\n",
    "THRESHOLD = -30\n",
    "CRISIS_RESOURCES = \"\"\"\n",
    "ðŸ“˜ HELPFUL MENTAL HEALTH RESOURCES (Thapar-Oriented):\n",
    "\n",
    "â€¢ Thapar Institute Counseling Cell Info:\n",
    "  https://www.thapar.edu/index.php?cid=counselling-cell\n",
    "\n",
    "â€¢ Blog: Dealing with Exam Stress (Thapar Students' Perspective):\n",
    "  https://connect.thapar.edu/blog/dealing-with-exam-stress\n",
    "\n",
    "â€¢ Blog: Finding Balance â€“ A Student's Guide to Mental Health:\n",
    "  https://connect.thapar.edu/blog/student-mental-health-guide\n",
    "\n",
    "â€¢ iCall (TISS) Free Counseling via Phone or Email:\n",
    "  https://icallhelpline.org/\n",
    "\n",
    "ðŸ’¡ If you're in immediate distress, please reach out to a trusted friend, mentor, or faculty member.\n",
    "You're not alone, and help is always available.\n",
    "\"\"\"\n",
    "\n",
    "# ====================\n",
    "# Conversation State\n",
    "# ====================\n",
    "class ConversationState:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.score = 0\n",
    "        self.referred = False\n",
    "        self.crisis_count = 0\n",
    "        self.session_start = datetime.now()\n",
    "\n",
    "    def update(self, user_input, bot_reply, emotion, sentiment):\n",
    "        score = EMOTION_SENTIMENT_SCORE.get((emotion, sentiment), 0)\n",
    "        self.score += score\n",
    "        self.history.append({\n",
    "            \"user\": user_input,\n",
    "            \"bot\": bot_reply,\n",
    "            \"emotion\": emotion,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"score\": score,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Keep only last 10 exchanges to prevent memory issues\n",
    "        if len(self.history) > 10:\n",
    "            self.history.pop(0)\n",
    "            \n",
    "        return score\n",
    "\n",
    "    def check_crisis(self, text):\n",
    "        text_lower = text.lower()\n",
    "        for keyword in CRISIS_KEYWORDS:\n",
    "            if keyword in text_lower:\n",
    "                self.crisis_count += 1\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def log_referral(self):\n",
    "        try:\n",
    "            os.makedirs(\"logs\", exist_ok=True)\n",
    "            with open(\"logs/support_referrals.log\", \"a\", encoding='utf-8') as logf:\n",
    "                logf.write(f\"[{datetime.now().isoformat()}] URGENT SUPPORT REFERRAL\\n\")\n",
    "                logf.write(f\"Session Duration: {datetime.now() - self.session_start}\\n\")\n",
    "                logf.write(f\"Cumulative Score: {self.score}\\n\")\n",
    "                logf.write(f\"Crisis Keywords Detected: {self.crisis_count}\\n\")\n",
    "                logf.write(f\"Recent History: {json.dumps(self.history[-3:], indent=2)}\\n\")\n",
    "                logf.write(\"-\" * 50 + \"\\n\\n\")\n",
    "            self.referred = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log referral: {e}\")\n",
    "\n",
    "# ====================\n",
    "# LLaMA Responder\n",
    "# ====================\n",
    "class LlamaResponder:\n",
    "    def __init__(self, model_path=\"models/Llama-3.2-3B-Instruct-Q5_K_S.gguf\"):\n",
    "        try:\n",
    "            self.model = Llama(\n",
    "                model_path=model_path,\n",
    "                n_gpu_layers=32,\n",
    "                n_ctx=2048,\n",
    "                use_mlock=True,\n",
    "                n_threads=8,\n",
    "                f16_kv=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            self.model_loaded = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load LLaMA model: {e}\")\n",
    "            self.model_loaded = False\n",
    "\n",
    "    def generate_response(self, user_input, emotion, sentiment, history):\n",
    "        if not self.model_loaded:\n",
    "            return \"I'm here to listen and support you. How are you feeling right now?\"\n",
    "        \n",
    "        try:\n",
    "            # Build context from recent history\n",
    "            context = \"\"\n",
    "            if history:\n",
    "                recent_history = history[-3:]  # Last 3 exchanges\n",
    "                for exchange in recent_history:\n",
    "                    context += f\"User: {exchange['user']}\\nAI: {exchange['bot']}\\n\"\n",
    "            \n",
    "            # Enhanced prompt with emotion awareness\n",
    "            prompt = f\"\"\"You are a compassionate, trained mental health supporter. Be warm, empathetic, and supportive.\n",
    "\n",
    "Current user emotion: {emotion}\n",
    "Current sentiment: {sentiment}\n",
    "\n",
    "{context}\n",
    "User: {user_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "            output = self.model(\n",
    "                prompt.strip(), \n",
    "                max_tokens=150, \n",
    "                stop=[\"User:\", \"AI:\", \"\\n\\n\"], \n",
    "                echo=False,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "            response = output[\"choices\"][0][\"text\"].strip()\n",
    "            \n",
    "            # Ensure response isn't empty\n",
    "            if not response:\n",
    "                return \"I hear you, and I want you to know that your feelings are valid. Can you tell me more about what's on your mind?\"\n",
    "                \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {e}\")\n",
    "            return \"I'm here for you. Sometimes it helps to talk about what's bothering you. How can I support you today?\"\n",
    "\n",
    "# ====================\n",
    "# Emotion Sentiment Pipeline\n",
    "# ====================\n",
    "class EmotionSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            \"bert_emotion_model_path\": \"./models/emotions_model\",\n",
    "            \"bert_sentiment_model_path\": \"./models/sentiment_model\",\n",
    "            \"extended_emotion_classifier_path\": \"./models/logistic_classifier/extended_classifier.pkl\",\n",
    "            \"emotion_results_path\": \"emotion_model_results.json\",\n",
    "            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        }\n",
    "        self.sentiment_id_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "        self.models_loaded = False\n",
    "        self.load_models()\n",
    "\n",
    "    def load_models(self):\n",
    "        try:\n",
    "            # Load emotion model\n",
    "            self.emotion_tokenizer = AutoTokenizer.from_pretrained(self.config[\"bert_emotion_model_path\"])\n",
    "            self.emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.config[\"bert_emotion_model_path\"]).to(self.config[\"device\"]).eval()\n",
    "\n",
    "            # Load emotion mappings\n",
    "            with open(self.config[\"emotion_results_path\"], 'r') as f:\n",
    "                emotion_results = json.load(f)\n",
    "                self.emotion_id_to_label = {int(k): v for k, v in emotion_results[\"label_mappings\"][\"emotion_id_to_label\"].items()}\n",
    "\n",
    "            # Load sentiment model\n",
    "            self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.config[\"bert_sentiment_model_path\"])\n",
    "            self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.config[\"bert_sentiment_model_path\"]).to(self.config[\"device\"]).eval()\n",
    "\n",
    "            # Load extended classifier\n",
    "            self.extended_embedder = SentenceTransformer('paraphrase-MiniLM-L12-v2', device='cpu')\n",
    "            self.extended_classifier = joblib.load(self.config[\"extended_emotion_classifier_path\"])\n",
    "\n",
    "            # Initialize responder\n",
    "            self.llama_responder = LlamaResponder()\n",
    "            \n",
    "            self.models_loaded = True\n",
    "            print(\"All models loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {e}\")\n",
    "            print(f\"Model loading failed: {e}\")\n",
    "            self.models_loaded = False\n",
    "\n",
    "    def predict_base_emotion(self, text):\n",
    "        if not self.models_loaded:\n",
    "            return \"neutral\"\n",
    "            \n",
    "        try:\n",
    "            inputs = self.emotion_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.config[\"device\"]) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.emotion_model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                pred_id = torch.argmax(probs, dim=-1).item()\n",
    "            return self.emotion_id_to_label.get(pred_id, \"neutral\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting base emotion: {e}\")\n",
    "            return \"neutral\"\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        if not self.models_loaded:\n",
    "            return \"Neutral\"\n",
    "            \n",
    "        try:\n",
    "            inputs = self.sentiment_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "            inputs = {k: v.to(self.config[\"device\"]) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.sentiment_model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                pred_id = torch.argmax(probs, dim=-1).item()\n",
    "            return self.sentiment_id_to_label.get(pred_id, \"Neutral\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting sentiment: {e}\")\n",
    "            return \"Neutral\"\n",
    "\n",
    "    def predict_extended_emotion(self, text, base_emotion):\n",
    "        if not self.models_loaded:\n",
    "            return base_emotion\n",
    "            \n",
    "        try:\n",
    "            emb_input = f\"{base_emotion} [SEP] {text}\"\n",
    "            emb = self.extended_embedder.encode([emb_input])\n",
    "            pred = self.extended_classifier.predict(emb)[0]\n",
    "            return pred\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error predicting extended emotion: {e}\")\n",
    "            return base_emotion\n",
    "\n",
    "    def analyze_text(self, text, state):\n",
    "        # Handle empty input\n",
    "        if not text or not text.strip():\n",
    "            return \"I'm here when you're ready to talk. Take your time.\"\n",
    "        \n",
    "        # Crisis detection with immediate response\n",
    "        if state.check_crisis(text):\n",
    "            state.log_referral()\n",
    "            crisis_response = (\n",
    "                \"I'm really concerned about you right now. Your life has value, and there are people who want to help. \"\n",
    "                \"Please reach out to a crisis helpline or emergency services immediately.\\n\\n\" + CRISIS_RESOURCES\n",
    "            )\n",
    "            return crisis_response\n",
    "\n",
    "        # Emotion and sentiment analysis\n",
    "        base_emotion = self.predict_base_emotion(text)\n",
    "        extended_emotion = self.predict_extended_emotion(text, base_emotion)\n",
    "        sentiment = self.predict_sentiment(text)\n",
    "\n",
    "        # Generate response\n",
    "        bot_reply = self.llama_responder.generate_response(text, extended_emotion, sentiment, state.history)\n",
    "        \n",
    "        # Update state\n",
    "        score_change = state.update(text, bot_reply, extended_emotion, sentiment)\n",
    "\n",
    "        # Check for referral threshold\n",
    "        if state.score <= THRESHOLD and not state.referred:\n",
    "            state.log_referral()\n",
    "            referral_note = \"\\n\\nI notice you've been struggling. Consider reaching out to a mental health professional who can provide specialized support. You deserve care and help.\"\n",
    "            return bot_reply + referral_note\n",
    "\n",
    "        return bot_reply\n",
    "\n",
    "# ====================\n",
    "# Run Interactive Chat\n",
    "# ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('logs/chatbot_errors.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ§  Mental Health Support Assistant\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"I'm here to listen and support you.\")\n",
    "    print(\"Type 'quit' or 'exit' to end the conversation.\\n\")\n",
    "    \n",
    "    try:\n",
    "        pipeline = EmotionSentimentPipeline()\n",
    "        state = ConversationState()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"You: \").strip()\n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"AI: Take care of yourself. Remember, you're not alone. ðŸ’™\")\n",
    "                    break\n",
    "                    \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                    \n",
    "                response = pipeline.analyze_text(user_input, state)\n",
    "                print(f\"AI: {response}\\n\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nAI: Take care of yourself. Remember, you're not alone. ðŸ’™\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error in main loop: {e}\")\n",
    "                print(\"AI: I'm having some technical difficulties, but I'm still here for you. How are you feeling?\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Critical error initializing chatbot: {e}\")\n",
    "        print(f\"Sorry, I'm having trouble starting up. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cca1c-7497-4f3e-a67b-b5c5c50b5d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
